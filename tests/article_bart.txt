BART Model Explained - Understand the Architecture of BART for Text Generation Tasks like summarization, abstraction questions answering and others.
Generalizing BERT (due to the bidirectional encoder) and GPT2 (with the left to right decoder) - Enter the World of a Mysterious new Seq2Seq Model - BART Models
Downloadable solution code | Explanatory videos | Tech SupportStart Project
- What is the BART HuggingFace Transformer Model in NLP?
- What is BART Model used for?
- What Data was BART Model Trained on?
- How many Parameters does BART have?
- BART Architecture Explained
- BART Model for Text Summarization
- Machine Learning Project Ideas for Practice using BART
HuggingFace Transformer models provide an easy-to-use implementation of some of the best performing models in natural language processing. Transformer models are the current state-of-the-art (SOTA) in several NLP tasks such as text classification, text generation, text summarization, and question answering.
The original Transformer is based on an encoder-decoder architecture and is a classic sequence-to-sequence model. The model’s input and output are in the form of a sequence (text), and the encoder learns a high-dimensional representation of the input,which is then mapped to the output by the decoder. This architecture introduced a new form of learning for language-related tasks and, thus, the models spawned from it achieve outstanding results overtaking the existing deep neural network-based methods.
Since the inception of the vanilla Transformer, several recent models inspired by the Transformer used the architecture to improve the benchmark of NLP tasks. Transformer models are first pre-trained on a large text corpus (such as BookCorpus or Wikipedia). This pretraining makes sure that the model “understands language” and has a decent starting point to learn how to perform further tasks. Hence, after this step, we only have a language model. The ability of the model to understand language is highly significant since it will determine how well you can further train the model for something like text classification or text summarization.
BART is one such Transformer model that takes components from other Transformer models and improves the pretraining learning. BART or Bidirectional and Auto-Regressive
Transformers was proposed in the BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension paper. The BART HugggingFace model allows the pre-trained weights and weights fine-tuned on question-answering, text summarization, conditional text generation, mask filling, and sequence classification.
So without much ado, let's explore the BART model – the uses, architecture, working, as well as a HuggingFace example.
New Projects
As mentioned in the original paper, BART is a sequence-to-sequence model trained as a denoising autoencoder. This means that a fine-tuned BART model can take a text sequence (for example, English) as input and produce a different text sequence at the output (for example, French). This type of model is relevant for machine translation (translating text from one language to another), question-answering (producing answers for a given question on a specific corpus), text summarization (giving a summary of or paraphrasing a long text document), or sequence classification (categorizing input text sentences or tokens). Another task is sentence entailment which, given two or more sentences, evaluates whether the sentences are logical extensions or are logically related to a given statement.
Since the unsupervised pretraining of BART results in a language model, we can fine-tune this language model to a specific task in NLP. Because the model has already been pre-trained, fine-tuning does not need massive labeled datasets (relative to what one would need for training from scratch). The BART model can be fine-tuned to domain-specific datasets to develop applications such as medical conversational chatbots, converting natural text to programming code or SQL queries, context-specific language translation apps, or a tool to paraphrase research papers.
BART was trained as a denoising autoencoder, so the training data includes “corrupted” or “noisy” text, which would be mapped to clean or original text. The training format is similar to the training of any denoising autoencoder. Just how in computer vision, we train autoencoders to remove noise or improve the quality of an image by having noisy images in the training data mapped with clean, original images as the target.
So, what exactly counts as noise for text data? The authors of BART settle on using some existing and some new noising techniques for pretraining. The noising schemes they use are Token Masking, Token Deletion, Text Infilling, Sentence Permutation, and Document Rotation. Looking into each of these transformations:
-
Token Masking: Random tokens in a sentence are replaced with [MASK]. The model learns how to predict the single token based on the rest of the sequence.
-
Token Deletion: Random tokens are deleted. The model must learn to predict the token content and find the position where the token was deleted from.
-
Text Infilling: A fixed number of contiguous tokens are deleted and replaced with a single [MASK] token. The model must learn the content of the missing tokens and the number of tokens.
-
Sentence Permutation: Sentences (separated by full stops) are permuted randomly. This helps the model to learn the logical entailment of sentences.
-
Document Rotation: The document is rearranged to start with a random token. The content before the token is appended at the end of the document. This gives insights into how the document is typically arranged and how the beginning or ending of a document looks like.
However, not all transformations are employed in training the final BART model. Based on a comparative study of pre-training objectives, the authors use only text infilling and sentence permutation transformations, with about 30% of tokens being masked and all sentences permuted.
These transformations are applied to 160GB of text from the English Wikipedia and BookCorpus dataset. With this dataset, the vocabulary size is around 29000, and the maximum length of the sequences is 512 characters in the clean data.
Get Closer To Your Dream of Becoming a Data Scientist with 150+ Solved End-to-End ML Projects
BART is constructed from a bi-directional encoder like in BERT and an autoregressive decoder like GPT. BERT has around 110M parameters while GPT has 117M, such trainable weights. BART being a sequenced version of the two, fittingly has nearly 140M parameters. Many parameters are justified by the supreme performance it yields on several tasks compared to fine-tuned BERT or its variations like RoBERTa, which has 125M parameters in its base model.
Below we can see more details about the number of parameters in different BART models. We can look at RoBERTa and its number of parameters on similar tasks for comparison.
BART outperforms RoBERTa in several fine-tuning tasks, as detailed further in the paper. The number of parameters of the latter is as follows:
To understand the purpose and philosophy behind BART’s architecture, first, we need to understand the nature of the NLP tasks it set out to solve. In tasks, such as question answering and text summarization, that require natural language understanding (or NLU) it is imperative for our model to read the text as a whole and understand each token in the context of what came, both, before and after it. For instance, training a masked language model with the sentence “the man went to the dairy store to buy a gallon of milk” could have a sentence like this as input based on the nosing schemes we saw above:
“the man went to the [MASK] store to buy a gallon of milk”.
Now, for an NLU task, it is important for the model to read the sentence completely before predicting [MASK] since it highly depends on the terms like “store” and “milk”. In such a case, the input sequence can be properly interpreted and learned by a bi-directional approach to reading and representing the text. The BERT (or Bidirectional Encoder Representations from Transformers) model incorporates this idea to greatly improve the language modeling task that happens in pre-training.
Thus, the first part of BART uses the bi-directional encoder of BERT to find the best representation of its input sequence. For every text sequence in its input, the BERT encoder outputs an embedding vector for each token in the sequence as well as an additional vector containing sentence-level information. In this way, the decoder can learn for both token and sentence-level tasks making it a robust starting point for any future fine-tuning tasks.
The pre-training is done using the masked sequences as discussed previously and shown below. While BERT was trained by using a simple token masking technique, BART empowers the BERT encoder by using more challenging kinds of masking mechanisms in its pre-training.
Once we get the token and sentence-level representation of an input text sequence, a decoder needs to interpret these to map with the output target. However, by using a similarly designed decoder, tasks such as next sentence prediction or token prediction might perform poorly since the model relies on a more comprehensive input prompt. In these cases, we need model architectures that can be trained on generating the next word by only looking at the previous words in the sequence. Hence, a causal or autoregressive model that looks only at the past data to predict the future comes in handy.
The GPT-1 model used an architecture similar to the decoder segment of the vanilla Transformers. GPT sequentially stacks 12 such decoders such that learning from only the past tokens can affect the current token calculation. The architecture is shown above. As seen in the original Transformer decoder, the GPT decoder also uses a masked multiheaded self-attention block and a feed-forward layer.
Get FREE Access to Machine Learning Example Codes for Data Cleaning, Data Munging, and Data Visualization
Comparable to other models we discussed here, including BART, GPT also takes a semi-supervised approach to learning. First, the model is pre-trained on tokens “t” looking back to “k” tokens in the past to compute the current token. This is done unsupervised on a vast text corpus to allow the model to “learn the language.”
Next, to make the model robust on a specific task, it is fine-tuned in a supervised manner to maximize the likelihood of label “y” given feature vectors x1…xn.
Combining 1 and 2, we get the objective in 3. Lambda represents a learned weight parameter to control the influence of language modeling.
Below image shows how the autoregressive decoder processes its input.
Although we separate the decoder from an encoder, the input to the decoder would still be a learned representation (or embedding) of the original text sequence. Thus, BART attaches the bi-directional encoder to the autoregressive decoder to create a denoising auto-encoder architecture. And based on these two components, the final BART model would look something like this:
In the above figure, the input sequence is a masked (or noisy) version of [ABCDE] transformed into [A[MASK]B[MASK]E]. The encoder looks at the entire sequence and learns high-dimensional representations with bi-directional information. The decoder takes these thought vectors and regressively predicts the next token. Learning occurs by computing and optimizing the negative log-likelihood as mapped with the target [ABCDE].
As more and more long-form content burgeons on the internet, it is becoming increasingly time-consuming for someone like a researcher or journalist to filter out the content they want. Summaries or paraphrase synopsis help readers quickly go through the highlights of a huge amount of textual content and save enough time to study the relevant documents.
Transformer models can automate this NLP task of text summarization. There are two approaches to achieve this: extractive and abstractive. Extractive summarization identifies and extracts the most significant statements from a given document as they are found in the text. This can be considered more of an information retrieval task. Abstractive summarization is more challenging as it aims to understand the entire document and generate paraphrased text to summarize the main points. Transformer models, including BART , perform the latter kind of summarization.
As we noted at the beginning of this article, HuggingFace provides access to both pre-trained and fine-tuned weights to thousands of Transformer models, BART being just one of them. For the text summarization task, you can choose fine-tuned BART models from the HuggingFace model explorer website. You can find the configuration and training description of every model uploaded there. Let’s check the bart-large-cnn model for beginners.
The model page: facebook/bart-large-cnn · Hugging Face also provides a hosted inference API instance for you to test text summarization on the text of your choice. For example, looking at the default text and running summarizer on it, we get:
Now, since this particular model has been fine-tuned on the CNN Daily Mail news clips dataset, the given text would undoubtedly perform well on this model. Let’s see what happens when we enter a different text. We tried the BART summarizer to summarize a paragraph from this article itself!
The performance is pretty decent! This shows how well the semi-supervised approach to train BART generalizes across domains to understand language.
Explore More Data Science and Machine Learning Projects for Practice. Fast-Track Your Career Transition with ProjectPro
Once you have picked a model, you can either download it directly from the model page or use the transformers package to integrate the model now with your Python application.
Firstly, run pip install transformers or follow the HuggingFace Installation page.
Next, you can build your summarizer in three simple steps:
First, load the model pipeline from transformers. Define the pipeline module by mentioning the task name and model name. We use “summarization” and the model as “facebook/bart-large-xsum”. Here, we can try for the Extreme Summary (XSum) dataset instead of the news dataset. The model has been trained to produce only single-sentence summaries. By checking on the default text provided out of the news dataset, we can check how different this new model is.
Thus, the final step is to define an input sequence and test it using the summarizer() pipeline. This function also takes max_length and min_length parameters, and these two control the summary length in terms of tokens.
As we see, the performance is excellent. However, things change if we add a custom text sequence.
Here the summary is less than perfect but is still accurately pertaining to the few introducing lines.
Another approach would be to use BartTokenizer to make tokens from the text sequences and BartForConditionalGeneration for the summarization.
Ace Your Next Job Interview with Mock Interviews from Experts to Improve Your Skills and Boost Confidence!
-
Speech-to-Text or Automatic Speech Recognition (ASR)
With ASR, humans can communicate with computers and applications using their everyday speech. The ASR system will interpret the speech and convert it to an accurate text transcript. Further, the text transcript can be read and understood by a language model to perform various tasks such as a Google search, placing a reminder, /or playing a particular song.
Since speech and text are data sequences, they can be mapped by fine-tuning a seq2seq model such as BART. For starters, you can head on to the HuggingFace Speech2Text model and try their inference APIs to choose the best model for your use case. Alternatively, you can fine-tune your own BART model using datasets such as the LibriSpeech dataset or choose from the many available at jim-schwoebel/voice_datasets. Since audio and text data are not of the same format, some data processing would be required to separate acoustic segments and map each of them to text tokens. Once such a mapping is established, you can fine-tune the seq2seq model .
-
Extractive Summarization of Scholarly Articles
Researching a particular topic can be tedious since it must always start with an extensive literature survey of existing works. However, if you have narrowed down your problem, it can be pretty time-consuming to search for relevant research papers – since, despite the abstracts, one has to read through every paper to confirm whether it is a useful source.
You can fine-tune Seq2Seq Transformer models on scholarly articles in specific domains such as AI or COVID-19 to generate summaries. For instance, the Cornell Newsroom database or the KP20k dataset provides a many generic training samples while datasets such as ScisummNet are summaries of domain-specific literature. BART can be fine-tuned on these datasets to generate context-aware, extractive paraphrasing for long research papers.
Most Watched Projects
-
Generating SQL queries from Natural Text
Efficient SQL queries are paramount to retrieving information from large databases, yet it can be cumbersome for a user, especially a layperson, to generate quick retrieval results. Seq2Seq models can help translate natural questions such as “how many people in the table are women?” into executable SQL queries that can fetch the answers from the database.
Datasets such as SPIDER, SParC, and WikiSQL are prepared for this exact fine-tuning task. Transformer-powered models have recently achieved benchmarks on these datasets, and you can also achieve similar performance on your custom dataset. By simply fine-tuning the BART model, it is possible to generate SQL-like queries that are efficient and produce the expected results from the database.
With these bart model project ideas and an in-depth look at the sequence-to-sequence BART model, you can now confidently look into the HuggingFace APIs to not just test but also fine-tune it for your business use case. This article first looked at the problem BART aims to solve and the approach behind its superior performance, including the BART architecture and training data. We also looked at a demo inference for text summarization using BART’s Python implementation on HuggingFace. With this overview of theory and code, you have a perfect headstart into building a robust Transformer based seq2seq model in Python for your next machine learning project.